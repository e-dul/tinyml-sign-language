{"cells":[{"cell_type":"markdown","metadata":{"id":"hRTa3Ee15WsJ"},"source":["# ASL gestures recognizer\n","\n","\n","**Acknowledgements**\n","\n","Adapted from https://www.kaggle.com/code/benedar/mediapipe-model-for-asl/notebook\n"]},{"cell_type":"code","source":["!pip install --upgrade pip\n","!pip install mediapipe-model-maker"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wpSwunlYmwt1","executionInfo":{"status":"ok","timestamp":1704734979847,"user_tz":-60,"elapsed":45285,"user":{"displayName":"Emil Dulęba","userId":"12121070355527118001"}},"outputId":"49025f08-5886-4d9e-aa36-8c32ca320dcb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n","Collecting pip\n","  Downloading pip-23.3.2-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 23.1.2\n","    Uninstalling pip-23.1.2:\n","      Successfully uninstalled pip-23.1.2\n","Successfully installed pip-23.3.2\n","Collecting mediapipe-model-maker\n","  Downloading mediapipe_model_maker-0.2.1.3-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (1.4.0)\n","Collecting mediapipe>=0.10.0 (from mediapipe-model-maker)\n","  Downloading mediapipe-0.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (1.23.5)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (4.8.0.76)\n","Requirement already satisfied: tensorflow>=2.10 in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (2.15.0)\n","Collecting tensorflow-addons (from mediapipe-model-maker)\n","  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (4.9.4)\n","Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (0.15.0)\n","Collecting tf-models-official>=2.13.1 (from mediapipe-model-maker)\n","  Downloading tf_models_official-2.15.0-py2.py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (23.1.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (23.5.26)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (3.7.1)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (4.8.0.76)\n","Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (3.20.3)\n","Collecting sounddevice>=0.4.4 (from mediapipe>=0.10.0->mediapipe-model-maker)\n","  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (1.6.3)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (16.0.6)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (23.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (0.35.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (1.60.0)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (2.15.1)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (2.15.0)\n","Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (3.0.7)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (9.4.0)\n","Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (0.5.0)\n","Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (2.84.0)\n","Collecting immutabledict (from tf-models-official>=2.13.1->mediapipe-model-maker)\n","  Downloading immutabledict-4.1.0-py3-none-any.whl.metadata (3.2 kB)\n","Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (1.5.16)\n","Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (4.1.3)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (4.8.1.78)\n","Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (1.5.3)\n","Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (5.9.5)\n","Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (9.0.0)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (2.0.7)\n","Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (6.0.1)\n","Collecting sacrebleu (from tf-models-official>=2.13.1->mediapipe-model-maker)\n","  Downloading sacrebleu-2.4.0-py3-none-any.whl.metadata (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (1.11.4)\n","Collecting sentencepiece (from tf-models-official>=2.13.1->mediapipe-model-maker)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting seqeval (from tf-models-official>=2.13.1->mediapipe-model-maker)\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting tensorflow-model-optimization>=0.4.1 (from tf-models-official>=2.13.1->mediapipe-model-maker)\n","  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl.metadata (914 bytes)\n","Collecting tensorflow-text~=2.15.0 (from tf-models-official>=2.13.1->mediapipe-model-maker)\n","  Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n","Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (1.1.0)\n","Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons->mediapipe-model-maker)\n","  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (8.1.7)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (0.1.8)\n","Requirement already satisfied: etils>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->mediapipe-model-maker) (1.6.0)\n","Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (2.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (2.31.0)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (1.14.0)\n","Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (0.10.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (4.66.1)\n","Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (0.5.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.10->mediapipe-model-maker) (0.42.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->mediapipe-model-maker) (2023.6.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->mediapipe-model-maker) (6.1.1)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->mediapipe-model-maker) (3.17.0)\n","Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (0.22.0)\n","Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (2.17.3)\n","Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (0.1.1)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (2.11.1)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (4.1.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (2023.11.17)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (2.8.2)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (8.0.1)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (2.0.7)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (6.1.0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.22.0->tf-models-official>=2.13.1->mediapipe-model-maker) (2023.3.post1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->mediapipe-model-maker) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->mediapipe-model-maker) (3.6)\n","Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe>=0.10.0->mediapipe-model-maker) (1.16.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.10->mediapipe-model-maker) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.10->mediapipe-model-maker) (3.5.1)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.10->mediapipe-model-maker) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.10->mediapipe-model-maker) (3.0.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (3.1.1)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.13.1->mediapipe-model-maker) (0.5.1)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.13.1->mediapipe-model-maker) (0.3.0)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.13.1->mediapipe-model-maker) (4.9)\n","Collecting portalocker (from sacrebleu->tf-models-official>=2.13.1->mediapipe-model-maker)\n","  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official>=2.13.1->mediapipe-model-maker) (2023.6.3)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official>=2.13.1->mediapipe-model-maker) (0.9.0)\n","Collecting colorama (from sacrebleu->tf-models-official>=2.13.1->mediapipe-model-maker)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official>=2.13.1->mediapipe-model-maker) (4.9.4)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->tf-models-official>=2.13.1->mediapipe-model-maker) (1.2.2)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets->mediapipe-model-maker) (1.62.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe>=0.10.0->mediapipe-model-maker) (2.21)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (5.3.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.10->mediapipe-model-maker) (1.3.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.13.1->mediapipe-model-maker) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.13.1->mediapipe-model-maker) (3.2.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow>=2.10->mediapipe-model-maker) (2.1.3)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (0.5.1)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (1.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.10->mediapipe-model-maker) (3.2.2)\n","Downloading mediapipe_model_maker-0.2.1.3-py3-none-any.whl (127 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mediapipe-0.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tf_models_official-2.15.0-py2.py3-none-any.whl (2.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading immutabledict-4.1.0-py3-none-any.whl (4.5 kB)\n","Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=d8788c3ccf112074a7d6fda2cd8c5cc14c9e5ba7eb78d5068a3965ad5570484c\n","  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n","Successfully built seqeval\n","Installing collected packages: sentencepiece, typeguard, tensorflow-model-optimization, portalocker, immutabledict, colorama, tensorflow-addons, sounddevice, sacrebleu, seqeval, mediapipe, tensorflow-text, tf-models-official, mediapipe-model-maker\n","Successfully installed colorama-0.4.6 immutabledict-4.1.0 mediapipe-0.10.9 mediapipe-model-maker-0.2.1.3 portalocker-2.8.2 sacrebleu-2.4.0 sentencepiece-0.1.99 seqeval-1.2.2 sounddevice-0.4.6 tensorflow-addons-0.23.0 tensorflow-model-optimization-0.7.5 tensorflow-text-2.15.0 tf-models-official-2.15.0 typeguard-2.13.3\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TqOt6Sv7AsMi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704734991232,"user_tz":-60,"elapsed":11394,"user":{"displayName":"Emil Dulęba","userId":"12121070355527118001"}},"outputId":"1f602126-7f36-49c8-ccad-2f57747e61f3"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n"]}],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import random\n","import shutil\n","import tensorflow as tf\n","import pathlib\n","import pandas as pd\n","import mediapipe as mp\n","import cv2\n","\n","from mediapipe_model_maker import gesture_recognizer\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{"id":"0GoKGm1duzgk"},"source":["## Build the dataset"]},{"cell_type":"markdown","metadata":{"id":"-Qz0xYcXg4CX"},"source":["### Download the dataset\n","> Training data set contains 87,000 images which are 200x200 pixels. There are 29 classes, of which 26 are for the letters A-Z and 3 classes for SPACE, DELETE and NOTHING.\n","\n","Data has to be ogranized into train, validation and test splits(original validation set is very small)."]},{"cell_type":"markdown","source":["Set access to kaggle"],"metadata":{"id":"nWt34NLwrS_X"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"elapsed":87132,"status":"ok","timestamp":1704735078356,"user":{"displayName":"Emil Dulęba","userId":"12121070355527118001"},"user_tz":-60},"id":"iw1X-aiympXR","outputId":"37f13952-8123-4126-9add-a12bde3858f5"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-fd67ce4d-716e-4073-9900-2fd301851387\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-fd67ce4d-716e-4073-9900-2fd301851387\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kaggle.json to kaggle.json\n","rm: cannot remove '/root/.kaggle': No such file or directory\n","Downloading asl-alphabet.zip to /content\n"," 99% 1.02G/1.03G [00:10<00:00, 177MB/s]\n","100% 1.03G/1.03G [00:10<00:00, 101MB/s]\n"]}],"source":["# https://www.kaggle.com/discussions/general/74235\n","\n","from google.colab import files\n","files.upload()\n","\n","!rm -r ~/.kaggle\n","!mkdir ~/.kaggle\n","!mv ./kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json\n","\n","!kaggle datasets download -d grassknoted/asl-alphabet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iSe81ObBpbXx"},"outputs":[],"source":["! rm -rf asl_alphabet_train/\n","! rm -rf asl_alphabet_/\n","! unzip -o asl-alphabet.zip > /dev/null"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7GTW3ejgqXbz"},"outputs":[],"source":["! mv asl_alphabet_test/asl_alphabet_test/ asl_alphabet_test/asl_alphabet_test_2/\n","! rm -rf asl_alphabet_test/asl_alphabet_test/\n","! rm -rf asl_alphabet_test/asl_alphabet_validation/"]},{"cell_type":"code","source":["mp_drawing = mp.solutions.drawing_utils\n","mp_hands = mp.solutions.hands\n","\n","count = 0\n","\n","# for static images:\n","hands = mp_hands.Hands(\n","    static_image_mode=True,\n","    max_num_hands=2,\n","    min_detection_confidence=0.5)\n"],"metadata":{"id":"jt10UwSgnQ4l"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ro4oYaEmxe4r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704739833094,"user_tz":-60,"elapsed":4729483,"user":{"displayName":"Emil Dulęba","userId":"12121070355527118001"}},"outputId":"48f484af-3dc5-400c-e8ae-008649100b04"},"outputs":[{"output_type":"stream","name":"stdout","text":["J\n","Y\n","H\n","C\n","P\n","D\n","E\n","R\n","M\n","space\n","A\n","T\n","nothing\n","N\n","U\n","V\n","G\n","del\n","B\n","L\n","F\n","O\n","S\n","W\n","K\n","Z\n","X\n","Q\n","I\n"]}],"source":["train_dir = \"/content/asl_alphabet_train/asl_alphabet_train/\"\n","valid_dir = \"/content/asl_alphabet_train/asl_alphabet_validation/\"\n","test_dir =  \"/content/asl_alphabet_test/asl_alphabet_test/\"\n","\n","SEED = 123\n","random.seed(SEED)\n","\n","skip_labels = [\"nothing\"]\n","none_path = os.path.join(train_dir, \"nothing\")\n","\n","for label in os.listdir(train_dir):\n","    print(label)\n","    if label in skip_labels:\n","      continue\n","    src = os.path.join(train_dir, label)\n","    for files in os.listdir(src):\n","        img_path = os.path.join(src, files)\n","        image = cv2.imread(img_path)\n","\n","        # convert the BGR image to RGB before processing.\n","        results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n","\n","        if not results.multi_hand_landmarks:\n","            count = count + 1\n","            shutil.move(img_path, none_path)\n"]},{"cell_type":"code","source":["print(f\"no hand detected on {count} images\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AXibIZYYpelg","executionInfo":{"status":"ok","timestamp":1704739833095,"user_tz":-60,"elapsed":13,"user":{"displayName":"Emil Dulęba","userId":"12121070355527118001"}},"outputId":"d460574d-4426-412b-8bb8-7ff9ffe0c88e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["no hand detected on 20172 images\n"]}]},{"cell_type":"code","source":["! mv asl_alphabet_train/asl_alphabet_train/nothing asl_alphabet_train/asl_alphabet_train/none"],"metadata":{"id":"TaukUstd36_b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yO1Q2JaW5sIy"},"source":["Now that we have built the dataset lets view the first nine images and labels from the training set:"]},{"cell_type":"code","source":["print(train_dir)\n","labels = []\n","for i in os.listdir(train_dir):\n","  if os.path.isdir(os.path.join(train_dir, i)):\n","    labels.append(i)\n","\n","print(sorted(labels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Xcp4QYupqTc","executionInfo":{"status":"ok","timestamp":1704739833096,"user_tz":-60,"elapsed":8,"user":{"displayName":"Emil Dulęba","userId":"12121070355527118001"}},"outputId":"a118cc63-9db8-4625-a193-d8ecb4f16ebf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/asl_alphabet_train/asl_alphabet_train/\n","['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'none', 'space']\n"]}]},{"cell_type":"code","source":["NUM_EXAMPLES = 3\n","\n","for label in labels[:5]:\n","  label_dir = os.path.join(train_dir, label)\n","  example_filenames = os.listdir(label_dir)[:NUM_EXAMPLES]\n","  fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10,2))\n","  for i in range(NUM_EXAMPLES):\n","    axs[i].imshow(plt.imread(os.path.join(label_dir, example_filenames[i])))\n","    axs[i].get_xaxis().set_visible(False)\n","    axs[i].get_yaxis().set_visible(False)\n","  fig.suptitle(f'Showing {NUM_EXAMPLES} examples for {label}')\n","\n","plt.show()"],"metadata":{"id":"DmIpRoNop38y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = gesture_recognizer.Dataset.from_folder(\n","    dirname=train_dir,\n","    hparams=gesture_recognizer.HandDataPreprocessingParams()\n",")\n","train_data, rest_data = data.split(0.8)\n","validation_data, test_data = rest_data.split(0.5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YeF0jo7rq_4b","executionInfo":{"status":"ok","timestamp":1704744604440,"user_tz":-60,"elapsed":4770501,"user":{"displayName":"Emil Dulęba","userId":"12121070355527118001"}},"outputId":"72dfa42d-d3a4-4c27-bb96-1ce989d1f69f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://storage.googleapis.com/mediapipe-assets/palm_detection_full.tflite to /tmp/model_maker/gesture_recognizer/palm_detection_full.tflite\n","Downloading https://storage.googleapis.com/mediapipe-assets/hand_landmark_full.tflite to /tmp/model_maker/gesture_recognizer/hand_landmark_full.tflite\n","Downloading https://storage.googleapis.com/mediapipe-assets/gesture_embedder.tar.gz to /tmp/model_maker/gesture_recognizer/gesture_embedder\n"]}]},{"cell_type":"code","source":["hparams = gesture_recognizer.HParams(export_dir=\"exported_model\", epochs=15)\n","options = gesture_recognizer.GestureRecognizerOptions(hparams=hparams)\n","model = gesture_recognizer.GestureRecognizer.create(\n","    train_data=train_data,\n","    validation_data=validation_data,\n","    options=options\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gy3nsZsHrCkz","executionInfo":{"status":"ok","timestamp":1704746561580,"user_tz":-60,"elapsed":1956211,"user":{"displayName":"Emil Dulęba","userId":"12121070355527118001"}},"outputId":"1ae785bd-beee-4d40-ef91-25678c7ea0ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," hand_embedding (InputLayer  [(None, 128)]             0         \n"," )                                                               \n","                                                                 \n"," batch_normalization (Batch  (None, 128)               512       \n"," Normalization)                                                  \n","                                                                 \n"," re_lu (ReLU)                (None, 128)               0         \n","                                                                 \n"," dropout (Dropout)           (None, 128)               0         \n","                                                                 \n"," custom_gesture_recognizer_  (None, 29)                3741      \n"," out (Dense)                                                     \n","                                                                 \n","=================================================================\n","Total params: 4253 (16.61 KB)\n","Trainable params: 3997 (15.61 KB)\n","Non-trainable params: 256 (1.00 KB)\n","_________________________________________________________________\n","None\n","Epoch 1/15\n","23882/23882 [==============================] - 142s 6ms/step - loss: 0.8573 - categorical_accuracy: 0.7032 - val_loss: 0.2435 - val_categorical_accuracy: 0.9045 - lr: 0.0010\n","Epoch 2/15\n","23882/23882 [==============================] - 133s 6ms/step - loss: 0.6837 - categorical_accuracy: 0.7623 - val_loss: 0.2385 - val_categorical_accuracy: 0.9137 - lr: 9.9000e-04\n","Epoch 3/15\n","23882/23882 [==============================] - 119s 5ms/step - loss: 0.6654 - categorical_accuracy: 0.7672 - val_loss: 0.2429 - val_categorical_accuracy: 0.9166 - lr: 9.8010e-04\n","Epoch 4/15\n","23882/23882 [==============================] - 117s 5ms/step - loss: 0.6548 - categorical_accuracy: 0.7717 - val_loss: 0.2427 - val_categorical_accuracy: 0.9029 - lr: 9.7030e-04\n","Epoch 5/15\n","23882/23882 [==============================] - 134s 6ms/step - loss: 0.6503 - categorical_accuracy: 0.7744 - val_loss: 0.2380 - val_categorical_accuracy: 0.9104 - lr: 9.6060e-04\n","Epoch 6/15\n","23882/23882 [==============================] - 115s 5ms/step - loss: 0.6455 - categorical_accuracy: 0.7747 - val_loss: 0.2457 - val_categorical_accuracy: 0.9049 - lr: 9.5099e-04\n","Epoch 7/15\n","23882/23882 [==============================] - 120s 5ms/step - loss: 0.6444 - categorical_accuracy: 0.7756 - val_loss: 0.2492 - val_categorical_accuracy: 0.9025 - lr: 9.4148e-04\n","Epoch 8/15\n","23882/23882 [==============================] - 132s 6ms/step - loss: 0.6388 - categorical_accuracy: 0.7769 - val_loss: 0.2543 - val_categorical_accuracy: 0.9025 - lr: 9.3207e-04\n","Epoch 9/15\n","23882/23882 [==============================] - 115s 5ms/step - loss: 0.6375 - categorical_accuracy: 0.7788 - val_loss: 0.2634 - val_categorical_accuracy: 0.8945 - lr: 9.2274e-04\n","Epoch 10/15\n","23882/23882 [==============================] - 128s 5ms/step - loss: 0.6329 - categorical_accuracy: 0.7803 - val_loss: 0.2624 - val_categorical_accuracy: 0.9002 - lr: 9.1352e-04\n","Epoch 11/15\n","23882/23882 [==============================] - 131s 6ms/step - loss: 0.6332 - categorical_accuracy: 0.7798 - val_loss: 0.2626 - val_categorical_accuracy: 0.8950 - lr: 9.0438e-04\n","Epoch 12/15\n","23882/23882 [==============================] - 112s 5ms/step - loss: 0.6302 - categorical_accuracy: 0.7813 - val_loss: 0.2692 - val_categorical_accuracy: 0.8937 - lr: 8.9534e-04\n","Epoch 13/15\n","23882/23882 [==============================] - 113s 5ms/step - loss: 0.6314 - categorical_accuracy: 0.7804 - val_loss: 0.2718 - val_categorical_accuracy: 0.8915 - lr: 8.8638e-04\n","Epoch 14/15\n","23882/23882 [==============================] - 113s 5ms/step - loss: 0.6293 - categorical_accuracy: 0.7831 - val_loss: 0.2665 - val_categorical_accuracy: 0.8947 - lr: 8.7752e-04\n","Epoch 15/15\n","23882/23882 [==============================] - 116s 5ms/step - loss: 0.6270 - categorical_accuracy: 0.7824 - val_loss: 0.2747 - val_categorical_accuracy: 0.8896 - lr: 8.6875e-04\n"]}]},{"cell_type":"code","source":["loss, acc = model.evaluate(test_data, batch_size=1)\n","print(f\"Test loss:{loss}, Test accuracy:{acc}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Frviz0RLrK2z","executionInfo":{"status":"ok","timestamp":1704746647469,"user_tz":-60,"elapsed":41858,"user":{"displayName":"Emil Dulęba","userId":"12121070355527118001"}},"outputId":"e46f7b3a-174c-40f3-820e-8889a7764bc6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["5971/5971 [==============================] - 35s 2ms/step - loss: 0.2473 - categorical_accuracy: 0.9055\n","Test loss:0.2473103255033493, Test accuracy:0.9055434465408325\n"]}]},{"cell_type":"code","source":["model.export_model()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FulFW8UurLY1","executionInfo":{"status":"ok","timestamp":1704746666205,"user_tz":-60,"elapsed":2582,"user":{"displayName":"Emil Dulęba","userId":"12121070355527118001"}},"outputId":"8b571c23-7592-47a5-b66f-1a16e1f688d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://storage.googleapis.com/mediapipe-assets/gesture_embedder.tflite to /tmp/model_maker/gesture_recognizer/gesture_embedder.tflite\n","Using existing files at /tmp/model_maker/gesture_recognizer/palm_detection_full.tflite\n","Using existing files at /tmp/model_maker/gesture_recognizer/hand_landmark_full.tflite\n","Downloading https://storage.googleapis.com/mediapipe-assets/canned_gesture_classifier.tflite to /tmp/model_maker/gesture_recognizer/canned_gesture_classifier.tflite\n"]}]},{"cell_type":"code","source":["!zip -r models_gesture_reco.zip exported_model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T0mnIW-VqxYa","executionInfo":{"status":"ok","timestamp":1704747092479,"user_tz":-60,"elapsed":787,"user":{"displayName":"Emil Dulęba","userId":"12121070355527118001"}},"outputId":"e52c550e-e3a5-41c6-d4b6-796875c0c564"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["updating: exported_model/ (stored 0%)\n","updating: exported_model/checkpoint (deflated 43%)\n","updating: exported_model/best_model_weights.index (deflated 56%)\n","updating: exported_model/best_model_weights.data-00000-of-00001 (deflated 9%)\n","updating: exported_model/gesture_recognizer.task (deflated 24%)\n","updating: exported_model/logs/ (stored 0%)\n","updating: exported_model/logs/validation/ (stored 0%)\n","updating: exported_model/logs/validation/events.out.tfevents.1704744706.cf7bbf4e349f.173.1.v2 (deflated 75%)\n","updating: exported_model/logs/train/ (stored 0%)\n","updating: exported_model/logs/train/events.out.tfevents.1704744605.cf7bbf4e349f.173.0.v2 (deflated 85%)\n","updating: exported_model/epoch_models/ (stored 0%)\n","updating: exported_model/epoch_models/model-0013.index (deflated 56%)\n","updating: exported_model/epoch_models/model-0002.index (deflated 56%)\n","updating: exported_model/epoch_models/model-0005.index (deflated 56%)\n","updating: exported_model/epoch_models/model-0010.data-00000-of-00001 (deflated 9%)\n","updating: exported_model/epoch_models/model-0012.index (deflated 56%)\n","updating: exported_model/epoch_models/model-0009.data-00000-of-00001 (deflated 9%)\n","updating: exported_model/epoch_models/checkpoint (deflated 40%)\n","updating: exported_model/epoch_models/model-0006.index (deflated 56%)\n","updating: exported_model/epoch_models/model-0014.data-00000-of-00001 (deflated 9%)\n","updating: exported_model/epoch_models/model-0004.data-00000-of-00001 (deflated 9%)\n","updating: exported_model/epoch_models/model-0007.data-00000-of-00001 (deflated 9%)\n","updating: exported_model/epoch_models/model-0006.data-00000-of-00001 (deflated 9%)\n","updating: exported_model/epoch_models/model-0012.data-00000-of-00001 (deflated 9%)\n","updating: exported_model/epoch_models/model-0003.index (deflated 56%)\n","updating: exported_model/epoch_models/model-0010.index (deflated 56%)\n","updating: exported_model/epoch_models/model-0004.index (deflated 56%)\n","updating: exported_model/epoch_models/model-0007.index (deflated 56%)\n","updating: exported_model/epoch_models/model-0011.index (deflated 55%)\n","updating: exported_model/epoch_models/model-0008.index (deflated 56%)\n","updating: exported_model/epoch_models/model-0015.data-00000-of-00001 (deflated 9%)\n","updating: exported_model/epoch_models/model-0005.data-00000-of-00001 (deflated 9%)\n","updating: exported_model/epoch_models/model-0013.data-00000-of-00001 (deflated 9%)\n","updating: exported_model/epoch_models/model-0009.index (deflated 56%)\n","updating: exported_model/epoch_models/model-0011.data-00000-of-00001 (deflated 9%)\n","updating: exported_model/epoch_models/model-0003.data-00000-of-00001 (deflated 9%)\n","updating: exported_model/epoch_models/model-0001.index (deflated 55%)\n","updating: exported_model/epoch_models/model-0014.index (deflated 56%)\n","updating: exported_model/epoch_models/model-0001.data-00000-of-00001 (deflated 9%)\n","updating: exported_model/epoch_models/model-0008.data-00000-of-00001 (deflated 9%)\n","updating: exported_model/epoch_models/model-0015.index (deflated 56%)\n","updating: exported_model/epoch_models/model-0002.data-00000-of-00001 (deflated 9%)\n","updating: exported_model/metadata.json (deflated 69%)\n"]}]},{"cell_type":"markdown","source":["# Export datasets"],"metadata":{"id":"YpUXLe-_9Dvp"}},{"cell_type":"code","source":["train_data.gen_tf_dataset().save(\"train_data\")\n","validation_data.gen_tf_dataset().save(\"validation_data\")\n","test_data.gen_tf_dataset().save(\"test_data\")"],"metadata":{"id":"ujhmT6fi0r-a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!zip -r datasets_postproc.zip train_data validation_data test_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zggT_5tb1y92","executionInfo":{"status":"ok","timestamp":1704747122586,"user_tz":-60,"elapsed":3095,"user":{"displayName":"Emil Dulęba","userId":"12121070355527118001"}},"outputId":"c3d60ef2-ede9-4f09-8301-d8fd0d608b3c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: train_data/ (stored 0%)\n","  adding: train_data/11448688883308010032/ (stored 0%)\n","  adding: train_data/11448688883308010032/00000000.shard/ (stored 0%)\n","  adding: train_data/11448688883308010032/00000000.shard/00000000.snapshot (deflated 30%)\n","  adding: train_data/dataset_spec.pb (deflated 30%)\n","  adding: train_data/snapshot.metadata (stored 0%)\n","  adding: validation_data/ (stored 0%)\n","  adding: validation_data/dataset_spec.pb (deflated 30%)\n","  adding: validation_data/10973640621223305708/ (stored 0%)\n","  adding: validation_data/10973640621223305708/00000000.shard/ (stored 0%)\n","  adding: validation_data/10973640621223305708/00000000.shard/00000000.snapshot (deflated 30%)\n","  adding: validation_data/snapshot.metadata (stored 0%)\n","  adding: test_data/ (stored 0%)\n","  adding: test_data/dataset_spec.pb (deflated 30%)\n","  adding: test_data/7295529981080186486/ (stored 0%)\n","  adding: test_data/7295529981080186486/00000000.shard/ (stored 0%)\n","  adding: test_data/7295529981080186486/00000000.shard/00000000.snapshot (deflated 30%)\n","  adding: test_data/snapshot.metadata (stored 0%)\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}